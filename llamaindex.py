# -*- coding: utf-8 -*-
"""llamaIndex.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rTnHrX-St-tZemFHXEtAfcd5IfQySO84

**bold text** # Base stuff
"""

# Run this block just for the first round
# ! pip install llama-index
# ! pip install llama-index-vector-stores-chroma
# ! pip install accelerate

# # ! pip install huggingface_hub
# # ! pip install llama-index-embeddings-huggingface
# # ! pip install llama-index-llms-huggingface

# ! pip install llama-index-llms-gemini
# ! pip install llama-index-embeddings-gemini

# from llama_index.embeddings.huggingface import HuggingFaceEmbedding
# from llama_index.llms.huggingface import HuggingFaceInferenceAPI, HuggingFaceLLM

from llama_index.llms.gemini import Gemini
from llama_index.embeddings.gemini import GeminiEmbedding

from llama_index.core import SimpleDirectoryReader
from llama_index.core.node_parser import SimpleNodeParser
from llama_index.core import VectorStoreIndex, StorageContext
from llama_index.core import Settings
from llama_index.core.indices.postprocessor import SentenceEmbeddingOptimizer
from llama_index.core.response.pprint_utils import pprint_response
from llama_index.vector_stores.chroma import ChromaVectorStore
from llama_index.core import Prompt
import chromadb

from huggingface_hub import InferenceClient, notebook_login

# from google.colab import drive

# Conncting to google drive
# drive.mount('./drive')

# from google.colab import drive
# drive.mount('/content/drive')

#collection_name IS THE NAME OF THE COLLECTION THAT SHOULD CHANGE TO VETOR
#VECTOR CHANGES OUR DATA INTO NUMERICAL DATAS
#COLLECTION IS THE VECTORED DATASET
chromadb_collection_name = "collection_name"
chromadb_path = "./"

# '''THESE TWO LINES GET THE DATABASE PATH BUT IT DOES NOT MAKE DATA BASE , AND
#AFTER GIVING THE COLLECION NDAME IT MAKES A DATABASE CONTAINING THE COLLECTIONS(DATASET)'''
db = chromadb.PersistentClient(path=chromadb_path)
chroma_collection = db.get_or_create_collection(name=chromadb_collection_name)

gemini_environment = "models/gemini-1.0-pro"
gemini_embedding_environment = "models/embedding-001"
gemini_api_key = "AIzaSyDftxKVKkoMf2yHdnvhNvbK7Lq9oT2hHeg"

llm = Gemini(model_name=gemini_environment, api_key=gemini_api_key)
embed_model = GeminiEmbedding(model_name=gemini_embedding_environment, api_key=gemini_api_key, embed_batch_size=100)

# #we should make token in our profile in order to use it as api key
# llm = HuggingFaceInferenceAPI(model_name="meta-llama/Meta-Llama-3-8B-Instruct", token="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")
# embed_model = HuggingFaceEmbedding(model_name="mixedbread-ai/mxbai-embed-large-v1")

# Give the huggingface connections to llama index settings

Settings.llm = llm
Settings.embed_model = embed_model

"""# Data prepration"""

#READING  THE DATASET
dir_reader = SimpleDirectoryReader('./dataset', filename_as_id=True)
documents = dir_reader.load_data(show_progress=True)
documents

#MAKING A DIVIDER FOR OUR DATASET AND WITH THIS,WE CAN DIVIDE OUR DATASET.
node_parser = SimpleNodeParser.from_defaults(chunk_size=512, chunk_overlap=20)

#GIVING THE DIVIDER TO LLAMA

Settings.node_parser = node_parser

#'''THE LINE IS FOR INTRODUCING THE COLLECTION(DATABASE) TO LLAMA'''
vector_store = ChromaVectorStore(chroma_collection=chroma_collection)
#storage context is for giving any database with any format to llama
storage_context = StorageContext.from_defaults(vector_store=vector_store)

# This method:
# 1. Recieves documents which has been read
# 2. Uses node_parser and split the documents into chunks
# 3. Embeds and takes the text of those chunks and turn them into vectores
# 4. At the end we have a big list of vectores
index = VectorStoreIndex.from_documents(documents=documents, storage_context=storage_context, show_progress=True)

"""# Data querying"""

# # Run this block if you haven't run the last section
# vector_store = ChromaVectorStore(chroma_collection=chroma_collection)
# index = VectorStoreIndex.from_vector_store(vector_store=vector_store)

#'''This, turns our indxed database into a query engine which is connected to llm
# How the query engine works:
# 1. Searches the query in database to find to relavent data
# 2. then sends relavent data and query together to the llm and returns the response
# '''
query_engine = index.as_query_engine()

query = "What is animal?"
response = query_engine.query(query)
print(response)

# # To check on database relavent content
# search_text = "what is the brain semtiment of people 2.5 years ago?"
# embedding = embed_model.get_text_embedding(search_text)
# results = chroma_collection.query(
#     query_embeddings=[embedding],
#     n_results=2
# )
# print(results)